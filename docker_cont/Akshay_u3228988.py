# -*- coding: utf-8 -*-
"""Akshay_u3228988.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ehPX6Wv3Tl7BesBOQ-Wn2QYHgKhM0XB

# ASSIGNMENT 1
## Predictive Modelling of Eating-out problem
### Prologue
    In this assignment, we work through Sydney's restaurant landscape in 2018. Harnessing a dataset with over 10,000 records, we'll utilize feature engineering, modeling,visualisation to analyse the restaurants in sydney with it's ratings, votes,locations and so on.\
    Geographical plot of individual cusisines will also be visualised along with ML models to predict and classify the restaurants for analysis.
### Dataset Description
    This dataset scraped from zomato has 14 columns and over 10,000 observations. The variables are as follows:
    - address: Restaurant's address (text)
    - cost: Average cost for two people in AUD (numeric)
    - cuisine: Cuisines served by the restaurant (list)
    - lat: Latitude (numeric)
    - link: URL (text)
    - lng: Longitude (numeric)
    - phone: Phone number (numeric)
    - rating_number: Restaurant rating (numeric)
    - rating_text: Restaurant rating (text)
    - subzone: Suburb in which the restaurant resides (text)
    - title: Restaurant's name (text)
    - type: Business type (list)
    - votes: Number of users who provided the rating (numeric)
    - groupon: Indicates if the restaurant promotes itself on Groupon.com (boolean)
"""



# Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Geopandas
import geopandas as gpd
from geopandas.tools import sjoin
import plotly.express as px
# Sklearn ML Libraries
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error
from sklearn.neural_network import MLPClassifier
import warnings
warnings.filterwarnings("ignore")

# Importing the csv file
restaurant_data = pd.read_csv("data/zomato_df_final_data.csv")
restaurant_data.head()

# Checking the shape of the data
restaurant_data.shape
restaurant_data.info()
restaurant_data.describe()

"""# PART A
### QUESTION 1
"""

# Convert string representation of list to actual list
restaurant_data['cuisine'] = restaurant_data['cuisine'].apply(eval)

# Expand lists and get unique values
unique_cuisines = restaurant_data.explode('cuisine')['cuisine'].unique()

unique_cuisines.sort()
unique_cuisines

len(unique_cuisines)

"""A total of 124 unique cuisines are being served in Australia

"""

# Getting top 3 suburbs with most restaurants
top_3_subzones = restaurant_data['subzone'].value_counts().head(3)
top_3_subzones

# Plotting the top 3 suburbs with most restaurants
plt.figure(figsize=(10, 6))
top_3_subzones.plot(kind='bar', color='brown')
plt.title('Top 3 Suburbs with the Highest Number of Restaurants')
plt.xlabel('Suburb')
plt.ylabel('Number of Restaurants')
plt.xticks(rotation=45)
plt.show()

"""### Justification of the given problem: Cheap vs Expensive restaurants against their ratings

By looking at the histogram plot, we can say that the statement "Restaurants with ‘excellent’ rating are mostly very expensive while those with ‘Poor’ rating are rarely expensive" is somewhat accurate. While it's not strictly true that most "Excellent" rated restaurants are very expensive, a substantial number do fall in the higher price categories. Similarly, "Poor" rated restaurants are predominantly in the lower to mid price range in this dataset.
<br>
Used hypothesis testing down below to delve into it little more
"""

# Filtering 'Excellent' and 'Poor' ratings
filtered_data = restaurant_data[restaurant_data['rating_text'].isin(['Excellent', 'Poor'])]

# Define a color scheme for each rating
colors = {
    'Excellent': 'green',
    'Poor': 'black',
}

plt.figure(figsize=(10, 6))

# Plotting histograms for 'Excellent' and 'Poor' ratings
for rating in ['Excellent', 'Poor']:
    plt.hist(filtered_data[filtered_data['rating_text'] == rating]['cost'].dropna(),  # dropna() ensures missing values are ignored
             bins=15,
             alpha=0.5,
             color=colors[rating],
             label=rating)

# Labels and legent for the plot
plt.xlabel('Average Cost for Two People (AUD)')
plt.ylabel('Restaurant Counts')
plt.title('Stacked Cost Histograms for Excellent and Poor Ratings')
plt.legend(loc='upper right')
plt.show()

# Delving into the statistics of the data for further analysis
# Filter data for restaurants with "Excellent" and "Poor" ratings
excellent_df = restaurant_data[restaurant_data['rating_text'] == 'Excellent']
poor_df = restaurant_data[restaurant_data['rating_text'] == 'Poor']

# Calculate mean and median costs for both groups
mean_excellent = excellent_df['cost'].mean()
median_excellent = excellent_df['cost'].median()
mean_poor = poor_df['cost'].mean()
median_poor = poor_df['cost'].median()
# Count the number of restaurants in each group
count_excellent = len(excellent_df)
count_poor = len(poor_df)

# Display the mean, median, and count for both groups
(count_excellent,mean_excellent, median_excellent), (count_poor,mean_poor, median_poor)

# COnducitng a hypothesis test to determine if the difference in means is statistically significant
# Importing the ttest_ind function
# H0: There is no difference in mean costs between "Excellent" and "Poor" rated restaurants.
# H1: There is a difference in mean costs between "Excellent" and "Poor" rated restaurants.
from scipy.stats import ttest_ind
t_stat, p_value = ttest_ind(excellent_df['cost'].dropna(), poor_df['cost'].dropna())

t_stat, p_value

"""Excellent restaurants have a mean value of **102$ for 51 restaurants**, and poor rateed restaurants have a mean value of **56$ for 209 restaurants**, for which price is half for poor rated compared to excellent.

Extremely low p-value(<0.05), confirms that the that there is a statistically significant difference in the mean costs between "Excellent" and "Poor" rated restaurants, which can be skewed because of the big difference between both the categories of restaurants.

Hence, based on this hypothesis test, we can conclude that "Excellent" rated restaurants tend to be more expensive than "Poor" rated restaurants.

### QUESTION 2: EDA
"""

# Plotting the distribution for the key variables
# Filtering out NaN values from 'type_list'
type_list = restaurant_data['type'].str.strip("[]").str.replace("'", "").str.split(", ")
type_list = type_list.dropna()

# Flattening the 'type' list to count each type
all_types = [type for sublist in type_list for type in sublist]
type_series = pd.Series(all_types)

# Re-plotting the exploratory analysis, including the Type distribution
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))
fig.suptitle('Exploratory Analysis of Key Variables', fontsize=16)

# Plot for Cost distribution
sns.histplot(restaurant_data['cost'], bins=50, kde=True, ax=axes[0, 0], color='skyblue')
axes[0, 0].set_title('Distribution of Cost')
axes[0, 0].set_xlabel('Cost')
axes[0, 0].set_ylabel('Frequency')

# Plot for Rating distribution
sns.histplot(restaurant_data['rating_number'], bins=50, kde=True, ax=axes[0, 1], color='salmon')
axes[0, 1].set_title('Distribution of Ratings')
axes[0, 1].set_xlabel('Rating')
axes[0, 1].set_ylabel('Frequency')

# Plot for Type distribution
type_series.value_counts().plot(kind='bar', ax=axes[1, 0], color='lightgreen')
axes[1, 0].set_title('Distribution of Restaurant Types')
axes[1, 0].set_xlabel('Type')
axes[1, 0].set_ylabel('Number of Restaurants')
axes[1, 0].tick_params(axis='x', rotation=45)

# Removing the unused subplot
fig.delaxes(axes[1,1])

# Adjust layout
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()

"""From the exploratory analysis of the key variables, we can observe:

**Cost Distribution:**
Most of the restaurants have a cost ranging between 0 and $100.
There's a noticeable skew, indicating that there are some very high-end, expensive restaurants, but they are rare.

**Rating Distribution:**
The majority of the restaurants have ratings between 3.5 and 5.
There are very few restaurants with ratings below 3.

**Type Distribution:**
"Casual Dining" is the most common type of restaurant, followed by "Café" and "Bar".
Other types like "Dessert Parlor", "Lounge", and "Fine Dining" are less common.

"""

# Ratings vs Cost
plt.figure(figsize=(12, 6))
sns.boxplot(data=restaurant_data, x='rating_text', y='cost')
plt.title('Distribution of Cost by Rating Text')

"""Ratings vs Cost shows that the excellent rated restaurants are heavily skewed, meaning some of the restaurants are very expensive, followed by very good rated ones"""

# Ratings vs Type
plt.figure(figsize=(12, 6))
sns.boxplot(data=restaurant_data, x='rating_text', y='rating_number')
plt.title('Distribution of Ratings by Rating Text')

cuisine_counts = restaurant_data.explode('cuisine')['cuisine'].value_counts()
top_10_cuisines = cuisine_counts.head(10)

plt.figure(figsize=(12,8))
plot = sns.barplot(y=top_10_cuisines.index, x=top_10_cuisines.values, palette="viridis")

# Adding the counts on the plot
for index, value in enumerate(top_10_cuisines.values):
    plot.text(value, index, str(value), color='black', ha="left", va="center")

plt.xlabel('Number f Occurrences')
plt.ylabel('Cuisines')
plt.title('Top 10 Cuisines in Sydney Restaurants')
plt.tight_layout()
plt.show()

"""Cafe and modern australian restaurnts are found most with sydney limits based on the plot"""

N = 20  # Show the top 20 locations
top_locations = restaurant_data['subzone'].value_counts().head(N)

plt.figure(figsize=(14, 8))
top_locations.plot(kind='bar', color='purple')
plt.title(f'Top {N} Locations with the Most Restaurants')
plt.xlabel('Location')
plt.ylabel('Number of Restaurants')
plt.xticks(rotation=45)
plt.show()

"""CBD, Surry hills and paramatta ahve the most restaurants within sydney limits

### QUESTION 3: CUISINE DENSITY MAP
"""

# Load the GeoJSON data into a GeoDataFrame
sydney_geojson = gpd.read_file('data/sydney.geojson')

# Display the first few rows of the data
sydney_geojson.head()

# Checking the imported geo data
sydney_geojson.plot(color='brown')
plt.title('Sydney GeoJSON Data')

# Coverting to string type and filling NaN values
restaurant_data['cuisine'] = restaurant_data['cuisine'].astype(str).fillna("Unknown")

def preprocess_data(restaurant_data):
    # Convert restaurant_data to a GeoDataFrame using the lat and lng columns with point geometre
    restaurant_data['geometry'] = gpd.points_from_xy(restaurant_data['lng'], restaurant_data['lat'])
    restaurant_data_geo = gpd.GeoDataFrame(restaurant_data, crs='EPSG:4326', geometry='geometry')
    restaurant_data['cuisine'] = restaurant_data['cuisine'].astype(str).fillna("Unknown")

    # Extracting and flattening the list of cuisines
    cuisines_list = restaurant_data['cuisine'].str.strip("[]").str.replace("'", "").str.split(", ")
    # Flattening the list into a single list of cuisines
    all_cuisines = [cuisine for sublist in cuisines_list for cuisine in sublist]
    unique_cuisines = set(all_cuisines)

    return restaurant_data_geo, unique_cuisines
# Create a mask to identify rows where the specified cuisine is present in the 'cuisine' column and
# count the occurrences of the specified cuisine by subzone using the mask
def calculate_cuisine_density(restaurant_data, cuisine):
    mask = restaurant_data['cuisine'].apply(lambda x: cuisine in x)
    return restaurant_data[mask]['subzone'].value_counts()

def show_cuisine_densitymap(restaurant_data_geo, sydney_gdf_path="data/sydney.geojson", cuisine='Chinese'):
    # Filtering the specified cuisine rows only
    cuisine_geo_data = restaurant_data_geo[restaurant_data_geo['cuisine'].str.contains(cuisine, case=False, na=False)]

    # Load the sydney.geojson file
    sydney_gdf = gpd.read_file(sydney_gdf_path)

    # Using spatial join to merge Sydney geojson dataframe with cuisine filtered rows
    joined_cuisine_geo_data = sjoin(cuisine_geo_data, sydney_gdf, how='left', predicate='within')

    # Using groupby function to get the cuisine counts and then naming the columns
    cuisine_geo_counts = joined_cuisine_geo_data.groupby('SSC_NAME').size()
    cuisine_geo_counts_df = cuisine_geo_counts.to_frame().reset_index()
    cuisine_geo_counts_df.columns = ['SSC_NAME', 'Counts']

    # Merging the cuisine count dataframe with the Sydney geojson dataframe for plotting
    cuisine_density_map_data = sydney_gdf.merge(cuisine_geo_counts_df, on='SSC_NAME', how='outer')

    # Plotting the map using plot function
    fig, ax = plt.subplots(figsize=(15, 10))
    sydney_gdf.boundary.plot(ax=ax, color='dimgray')
    cuisine_density_map_data.plot(column='Counts', ax=ax, cmap='viridis', legend=True,
                                  legend_kwds={'label': f"Number of {cuisine} Restaurants by Suburb"})
    ax.set_title(f"{cuisine} Cuisine Density in Sydney", fontdict={'fontsize': '15', 'fontweight' : '3'})
    plt.axis('off')
    plt.show()


restaurant_data_geo, unique_cuisines = preprocess_data(restaurant_data)
show_cuisine_densitymap(restaurant_data_geo, cuisine='Chinese')

# Getting cuisine input from user
cuisine_input = input("Enter a cuisine: ")
restaurant_data_geo, unique_cuisines = preprocess_data(restaurant_data)
show_cuisine_densitymap(restaurant_data_geo, cuisine=cuisine_input)

"""### Bonus Question

Plotting an interactive cuisine densitymap for Indian cuisine
"""

# Generating the cuisine_density dictionary, which will be used for interactive map(plotly express and choropleth is used)
cuisine_density_dict = {}

# Looping through all unique cuisines
for cuisine in unique_cuisines:
    # Calculate density for the current cuisine
    density_data = calculate_cuisine_density(restaurant_data_geo, cuisine)
    # Store the result in the dictionary
    cuisine_density_dict[cuisine] = density_data


def interactive_cuisine_densitymap(cuisine='Indian'):
    # Fetch the density data for the specified cuisine
    density_data = cuisine_density_dict.get(cuisine, None)
    if density_data is None:
        print(f"No data available for {cuisine} cuisine.")
        return

    # Merge the density data with the geojson data
    merged_gdf = sydney_geojson.set_index('SSC_NAME').join(density_data.rename(cuisine)).reset_index()

    # Plotting the interactive choropleth map
    fig = px.choropleth(merged_gdf, geojson=merged_gdf.geometry,
                        locations=merged_gdf.index,
                        color=cuisine,
                        hover_name="SSC_NAME",
                        hover_data=[cuisine],
                        title=f"{cuisine} Cuisine Density in Sydney",
                        color_continuous_scale="viridis",
                        labels={cuisine: f"Number of {cuisine} Restaurants"})

    fig.update_geos(fitbounds="locations", visible=False)
    fig.show()

# Display the interactive cuisine density map for Indian cuisine as a test
interactive_cuisine_densitymap(cuisine='Indian')

# getting input from the user for cuisine interactive map
cuisine_input = input("Enter a cuisine: ")
interactive_cuisine_densitymap(cuisine=cuisine_input)

"""Limitations of regular plots:
1) Cannot be zoomed,tilted or interacted
2) Doesn't show the data points within the plot when hovered over

Benefites of interactive plots:
1) Ability to hover, zoom, tilt
2) Can be dynamilcally updated to user's preference.

### 4.Tableau Dashboard
Submitted as a part of the attached zip file sumbisison

## PART B: PREDICTIVE MODELLING

### FEATURE ENGINEERING
#### DEALING WITH NA'S
"""

# Check for missing values in the dataset
missing_data = restaurant_data.isnull().sum()
data_types = restaurant_data.dtypes
missing_data, data_types

"""Since the missing values are different types, each has to be handled differently.\
     - cost, cost_2, votes can be imputed wirh median, since there arent many NA's.\
     - For rating_number and rating_text, since these are our target variables, we should drop the rows where they are missing.Imputing them can cause bias in the data\
     - lat and lng can be dropped completely, as imputing them might not be accurate.
     - For 'type', a placeholder like "Unknown" can be filled in.
     
"""

# Drop rows where rating_number and rating_text are missing
restaurant_data.dropna(subset=['rating_number', 'rating_text'], inplace=True)

# Drop rows where lat and lng are missing
restaurant_data.dropna(subset=['lat', 'lng'], inplace=True)

# Impute missing values in cost, cost_2, and votes with their respective medians
for col in ['cost', 'cost_2', 'votes']:
    median_value = restaurant_data[col].median()
    restaurant_data[col].fillna(median_value, inplace=True)

# Fill missing values in type with "Unknown"
restaurant_data['type'].fillna('Unknown', inplace=True)

# Check for missing values again
remaining_missing_values = restaurant_data.isnull().sum()
remaining_missing_values

"""### 2. LABEL ENCODING"""

# rating_text is encoded ordinally frome Excellent to poor
# gropon coloumn is binary encoded
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Binary encode 'groupon'
label_encoder = LabelEncoder()
onehot_encoder = OneHotEncoder()
restaurant_data['groupon'] = label_encoder.fit_transform(restaurant_data['groupon'])

# Ordinal encode 'rating_text'
rating_text_label_encoder = LabelEncoder()
rating_text_label_encoder.classes_ = np.array(['Poor', 'Average', 'Good', 'Very Good', 'Excellent', 'Not Rated'])
restaurant_data['rating_text_encoded'] = rating_text_label_encoder.transform(restaurant_data['rating_text'])

# Drop the original 'rating_text' column
restaurant_data.drop('rating_text', axis=1, inplace=True)

# Display the first few rows after modifications
restaurant_data.head()

# Correlation heatmap of the selected columns for the model building phase. Cols like lat lon, phone are not neededhere.
# Subset dataframe to selected columns
selected_cols_df = restaurant_data[['cost', 'rating_number', 'rating_text_encoded', 'votes', 'groupon']]

# Calculate correlations for the selected columns
selected_correlations = selected_cols_df.corr()

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(selected_correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

# Generate pair plot for the selected columns
sns.pairplot(selected_cols_df, palette='plasma', diag_kind='hist', hue = 'rating_text_encoded',)

plt.suptitle('Pair Plot of Selected Features', y = 1.02, fontsize = 18)
plt.show()

"""From teh above plot, it is safe to say that the cost and votes can be log transformed and groupon barely constitute to any kind of correlation factor."""

### Log transformation
restaurant_data['log_cost'] = np.log(restaurant_data['cost'])
restaurant_data['log_votes'] = np.log(restaurant_data['votes'])

# Selectinh columns
selected_cols_df = restaurant_data[['cost', 'log_cost', 'rating_number', 'votes', 'log_votes','rating_text_encoded']]

# Generate pair plot for the selected columns
sns.pairplot(selected_cols_df, palette='plasma', hue = 'rating_text_encoded',)
plt.suptitle('Pair Plot of Selected Features', y = 1.02, fontsize = 18)
plt.show()

"""This plot shows a better distribution of the data. The log transformation has helped to reduce the skewness of the data and will be used for model building."""

# Calculate correlations for the selected columns
selected_correlations = selected_cols_df.corr()

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(selected_correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

"""### II.REGRESSION
3.LINEAR REGRESSION
"""

# Feature set and target variable
X = restaurant_data[['votes', 'rating_text_encoded', 'cost']]
y = restaurant_data[['rating_number']]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Build the regression model
model_regression_1 = LinearRegression()
model_regression_1.fit(X_train, y_train)

# Predict and compute MSE
y_pred_1_updated = model_regression_1.predict(X_test)
mse_1 = mean_squared_error(y_test, y_pred_1_updated)
r2_1 = model_regression_1.score(X_test, y_test)
print(f"Metrics for regression without log columns:\n MSE",mse_1,"\n R2:", r2_1)

sns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Features')
plt.show()

# Using Log values
# Feature set and target variable
X = restaurant_data[['log_votes', 'rating_text_encoded', 'log_cost']]
y = restaurant_data[['rating_number']]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Build the regression model
model_regression_log = LinearRegression()
model_regression_log.fit(X_train, y_train)

# Predict and compute MSE
y_pred_2_updated = model_regression_log.predict(X_test)
mse_2 = mean_squared_error(y_test, y_pred_2_updated)
r2_2 = model_regression_log.score(X_test, y_test)
print(f"Metrics for regression with log columns:\n MSE",mse_2,"\n R2:", r2_2)

"""Here,MSE and R2 values improved after the usage of Log columns resulting in a better model results.

### 4. Using Gradient descent
"""

# Feature set and target variable
X = restaurant_data[['log_votes', 'rating_text_encoded', 'log_cost']]
y = restaurant_data['rating_number']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# SGDRegressor is sensitive to feature scaling, so it's typically recommended to scale the data
pipeline = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3))

model_regression_2 = pipeline.fit(X_train, y_train)

# Predict and compute MSE
y_pred_3 = model_regression_2.predict(X_test)
mse_3 = mean_squared_error(y_test, y_pred_3)
r2_3 = model_regression_2.score(X_test, y_test)
print(f"Metrics for regression using SGD:\n MSE", mse_3, "\n R2:", r2_3)

"""After applying Stochastic Gradient descent on the log values, the values of r2 and MSE don't show any significant changes. Tuning the hyperparameters and sclaing the features indidually can increase the metrics of it.

### III. Classification
"""

# Adjust binary target column for Class 1 and Class 2
restaurant_data['binary_rating'] = restaurant_data['rating_text_encoded'].apply(lambda x: "Class 1" if x <= 1 else "Class 2")
# Features and target
y = restaurant_data['binary_rating']
restaurant_data[['log_votes', 'rating_text_encoded', 'log_cost','binary_rating']]

# Defining functions for getting a confusion matrix and classification report
def plot_confusion_matrix(conf_matrix, title, cmap = "magma"):
    plt.figure(figsize=(4,4))
    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap =cmap, cbar=False, xticklabels=['Class 1', 'Class 2'],
                    yticklabels=['Class 1', 'Class 2'])
    plt.title(title)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

def display_classification_metrics(y_true, y_pred, class_names=["Class 1", "Class 2"]):
    accuracy = (y_true == y_pred).mean()
    print("Classification Accuracy : ", accuracy)
    print()
    print(classification_report(y_true, y_pred, target_names=class_names))

# Split into train and test sets again for classification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

X_train.shape, X_test.shape

# Logistic Regression
model_classification_3 = LogisticRegression(max_iter=1000, random_state=0)
model_classification_3.fit(X_train, y_train)
logistic_pred = model_classification_3.predict(X_test)
logistic_cm = confusion_matrix(y_test, logistic_pred)
# Display confusion matrix
plot_confusion_matrix(logistic_cm, "Logistic Regression")
# Display classification metrics
display_classification_metrics(y_test, logistic_pred)

# Random Forest
model_rf = RandomForestClassifier(random_state=0)
model_rf.fit(X_train, y_train)
rf_pred = model_rf.predict(X_test)
rf_cm = confusion_matrix(y_test, rf_pred)
plot_confusion_matrix(rf_cm, "Random Forest")
# Display classification metrics
display_classification_metrics(y_test, rf_pred)

# Support Vector Classifier
model_svc = SVC(random_state=0)
model_svc.fit(X_train, y_train)
svc_pred = model_svc.predict(X_test)
svc_cm = confusion_matrix(y_test, svc_pred)
plot_confusion_matrix(svc_cm, "Support Vector Classifier")
# Display classification metrics
display_classification_metrics(y_test, svc_pred)

# K-Nearest Neighbors
model_knn = KNeighborsClassifier()
model_knn.fit(X_train, y_train)
knn_pred = model_knn.predict(X_test)
knn_cm = confusion_matrix(y_test, knn_pred)
plot_confusion_matrix(knn_cm, "K-Nearest Neighbors")
# Display classification metrics
display_classification_metrics(y_test, knn_pred)

# Neural Network
model_nn = MLPClassifier()
model_nn.fit(X_train, y_train)
nn_pred = model_knn.predict(X_test)
nn_cm = confusion_matrix(y_test, nn_pred)
plot_confusion_matrix(knn_cm, "Neural Network")
# Display classification metrics
display_classification_metrics(y_test, nn_pred)

"""### 9.CONCLUSIONS

1) Classifier models tend to lean more towards higher rated restaurants more. this could be due to bias and imbalance in the dataset becasue of more high rated restaurants.
2) This could be handles by handling the imbalances by using advanced techniques.
3) Binning effect: By converting coninuous variables like rating,some values undergo binning which leades tp loss of data, creating bias in the model.
4) Converting  the predictions to class 1 and class 2 also makes the model better in classificaiton.
"""